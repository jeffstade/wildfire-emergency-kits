---
title: "Predicting Wildfire Risk in Southern California, and Developing an Application for Evacuation Preparedness"
author: "Jenna Epstein and Jeff Stern"
date: "December, 2020"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
```

# Introduction
In the event of a wildfire, there is nothing more important than a successful evacuation. Wildfires in California are bad, and they are particularly bad in and around Los Angeles County. For example, one fire alone in October 2019 resulted in more than 100,000 people being ordered to evacuate their homes. 

The California Department of Forestry and Fire Protection (CALFIRE) recommends that all residents assemble an emergency supply kit to be better prepared in the event of a fire evacuation. Emergency supply kits provide critical goods as well as educational materials, such as an evacuation route, that make it feasible for a household to evacuate in the event of a wildfire.

We want to work with the county to strategically distribute emergency supply kits to the right individuals.

To do this, we will develop an algorithm and an application. The algorithm will output a prioritized list of census tracts to target with an offer for a free emergency supply kit. Census tracts will be prioritized based in equal parts on the probability of a fire occurring and the Social Vulnerability Index score for that tract. This will help ensure that kits make it to, both, those most in-need and those that are most vulnerable to emergencies like wildfires. The algorithm will be accessible via an interactive application that allows a program manager to tweak inputs such as budget and cost of the kits. The program manager will be able to visualize the exact areas and total number of community members they’d be able to serve.

[[ Embed youtube video here ]] https://youtu.be/0vPQi7OHWeU

# Setup
```{r load libraries and functions, message=FALSE, warning=FALSE}
#libraries
library(tidyverse)
library(dplyr)
library(plyr)
library(tidyr)  
library(sf)
library(sp)
library(viridis)
library(spatstat)
library(rgdal)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(mapview)
library(leaflet)    
library(rgeos)
library(nngeo)
library(gstat)
library(tigris)   
library(pscl)
library(plotROC)
library(pROC)
library(scales)
library(lubridate)
library(caret)
library(jtools)
library(ggrepel)

root.dir = "C:/Users/jenna/Documents/GitHub/MUSA508-final/"

# nn function
nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
      as.data.frame(nn) %>%
      rownames_to_column(var = "thisPoint") %>%
      gather(points, point_distance, V1:ncol(.)) %>%
      arrange(as.numeric(thisPoint)) %>%
      group_by(thisPoint) %>%
      summarize(pointDistance = mean(point_distance)) %>%
      arrange(as.numeric(thisPoint)) %>% 
      dplyr::select(-thisPoint) %>%
      pull()
  
  return(output)  
}

#r cross validate function
crossValidate <- function(dataset, id, dependentVariable, indVariables) {

allPredictions <- data.frame()
cvID_list <- unique(dataset[[id]])

for (i in cvID_list) {

  thisFold <- i
  cat("This hold out fold is", thisFold, "\n")

  fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
                dplyr::select(id, geometry, indVariables, dependentVariable)
  fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
                dplyr::select(id, geometry, indVariables, dependentVariable)
  
  regression <-
    glm(n_fires_int ~ ., family = "poisson", 
      data = fold.train %>% 
      dplyr::select(-geometry, -id))
  
  thisPrediction <- 
    mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
  allPredictions <-
    rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}

#quintile breaks
qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

#themes and palettes
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}
palette2 <- c("#fcff93", "#f7b61f")


#getting discrete viridis inferno
library(scales)
#q_colors =  5
#v_colors =  viridis(q_colors, option = "B")
#print(v_colors)

paletteInferno5 <- c("#f6d746", "#f7b61f", "#c83233", "#51005b", "#000005")
paletteInferno8 <- c("#fcff93", "#f6d746", "#f7b61f", "#f06813", "#c83233", "#8c1750", "#51005b", "#1d0042")

```


# Data Wrangling and Feature Engineering
```{r include=FALSE}
# need this to standardize the crs - hide code block when running

#JENNA
fishnet_rasterfeatures <- st_read("C:/Users/jenna/Documents/GitHub/MUSA508-final/data/fishnet_veg_elev_imperv_nofires/fishnet_raster_features.shp")

#JEFF - change to your local path
#fishnet_veg_elev_imperv <- st_read("C:/Users/jenna/Documents/GitHub/MUSA508-final/data/fishnet_veg_elev_imperv_nofires/fishnet_raster_features.shp")

# fishnet adjustments
fishnet_rasterfeatures <- fishnet_rasterfeatures %>% dplyr::mutate(uniqueID = row_number())
fishnet_rasterfeatures$uniqueID <-as.integer(fishnet_rasterfeatures$uniqueID)

# rename some columns
fishnet_rasterfeatures <- fishnet_rasterfeatures %>%
  dplyr::rename(VEG_MAJORITY = MAJORITY) %>% 
  dplyr::rename(ELEVATION_MEAN = MEAN) %>% 
  dplyr::rename(IMPERV_SURFACE_PCT_MEAN = MEAN_1) %>%
  dplyr::select(-AREA, -MEAN_12) %>% 
  st_sf()

#no longer needed, but just in case need this to refactor:
#fishnet_veg_elev <- fishnet_veg_elev %>% as.factor(fishnet_veg_elev$VEG_MAJORITY)

# convert the numbers for vegetation types back to the categories
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==0] <- "agriculture"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==1] <- "barren_or_other"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==2] <- "conifer"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==3] <- "hardwood"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==4] <- "herbaceous"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==5] <- "shrub"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==6] <- "urban"
fishnet_rasterfeatures$VEG_MAJORITY[fishnet_rasterfeatures$VEG_MAJORITY==7] <- "water"

```


## California County Boundaries
Data is loaded for the state of California and its counties. Five counties along the coast make up the study area for this analysis: Santa Barbara, Ventura, Los Angeles, Orange, and Santa Monica. 


```{r counties, message=FALSE, warning=FALSE, results='hide'}
#all cali boundaries (counties)
counties <-  st_read("https://opendata.arcgis.com/datasets/8713ced9b78a4abb97dc130a691a8695_0.geojson") %>% st_transform(st_crs(fishnet_rasterfeatures))

#socal counties
socal_counties <- counties %>% filter(COUNTY_NAME == "Kern"| COUNTY_NAME == "Riverside" | COUNTY_NAME == "Los Angeles" | COUNTY_NAME == "Imperial" | COUNTY_NAME == "San Bernardino" | COUNTY_NAME == "Ventura"| COUNTY_NAME == "Orange" | COUNTY_NAME == "Santa Barbara" | COUNTY_NAME == "San Luis Obispo" | COUNTY_NAME == "San Diego") 

socal_counties <- socal_counties %>% filter(OBJECTID < 59)

socal_counties_coastal <- socal_counties %>% filter(COUNTY_NAME == "Los Angeles" | COUNTY_NAME == "Santa Barbara" | COUNTY_NAME == "San Diego" | COUNTY_NAME == "Ventura" | COUNTY_NAME == "Orange")


# centroids of southern california counties
centroids.socal_counties <- cbind(socal_counties_coastal, st_coordinates(st_centroid(socal_counties_coastal)))

```


```{r fishnet with counties, message=FALSE, warning=FALSE}
#join fishnet with counties
fishnet_features <-
  st_centroid(fishnet_rasterfeatures) %>%
    st_join(dplyr::select(socal_counties_coastal, COUNTY_NAME)) %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(fishnet_rasterfeatures, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

```


## Historical Fire Data: Perimeters and Presence of Fires

[Text here about the Outcome Variable and process of bringing in the California fire perimeters].

```{r cali fires, message=FALSE, warning=FALSE}
#all perimeters
all_cali_fires <- st_read("https://opendata.arcgis.com/datasets/e3802d2abf8741a187e73a9db49d68fe_2.geojson") %>% st_transform(st_crs(fishnet_features)) 
# last five years 
cali_fires_recent <- all_cali_fires %>% filter(YEAR_ > 2014)

#clipped fire perimeter data to southern california counties in ArcMap
#JENNA
fires_clipped <- st_read("C:/Users/jenna/Documents/GitHub/MUSA508-final/data/fires_clipped/fires_clipped.shp") %>% st_sf() %>% filter(YEAR_ > 2014)

#JEFF - add your local path
#fishnet_veg_elev <- st_read("C:/Users/jenna/Documents/GitHub/MUSA508-final/data/fires_clipped/fires_clipped.shp") 

#filter to only include previous 10 years
fires_clipped <- fires_clipped %>% st_transform(st_crs(fishnet_features))

```

The historical fire perimeter data from CALFIRE is clipped to the southern portion of California and filtered to only include fire perimeters from incidents between 2015-2019. With climate change, environmental conditions have been changing more rapidly over more recent years, so it will be more realistic to train and test on a smaller year range of data when developing the model;


```{r map of clipped fire perimeter data, message=FALSE, warning=FALSE}
#map
ggplot()+
  geom_sf(data=socal_counties_coastal, fill="gray", color="white") +
  geom_sf(data=socal_counties, fill="gray", color="white") +
  geom_sf(data=fires_clipped, fill="#f7b61f", color="#f06813", alpha=0.9) +
     geom_text(data = centroids.socal_counties, aes(X, Y, label = COUNTY_NAME), size = 3, fontface="bold")+
  labs(title = "Fire Perimeters in Southern California",
       subtitle="2015-2019",
       caption="Data Sources: CALFIRE, California State Geoportal") +
  mapTheme()
         
```


## Fire Data: Light Feature Engineering 
Here, some feature engineering is done to to calculate the number of fire intersections per fishnet cell and create a new count variable to explore, `n_fires_int`. Then, a binary numeric variable is created called `prev_fire` to indicating the historical presence of a fire (1) or not (0) at any point over the past five years.


```{r fire feature engineering}
## 1. intersections of fire perimeters with each fishnet cell. using length function: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths
fires_fishnet <- fishnet_features %>% mutate(n_fires_int = lengths(st_intersects(fishnet_features, fires_clipped)))

## 2. adding a column for yes or no for historical fire presence
fires_fishnet <-
  fires_fishnet %>%
  mutate(prev_fire = ifelse(fires_fishnet$n_fires_int > 0, "1", "0"))

```


### Map: Fire Incident Intersections per Fishnet Cell
This map shows the count of fire intersections per fishnet cell. Note that this was ultimately not used in model development, but it is interesting to explore the frequency of fire incidents through count data in this area. 

```{r intersections of fires by fishnet map, message=FALSE, warning=FALSE}
# mapping sum of fire incident intersections by cell
ggplot() +
  geom_sf(data = fires_fishnet, aes(fill = n_fires_int)) +
  scale_fill_viridis(option="B")+
  labs(title = "Fire Perimeter Intersections in Southern California",
       fill="Fire Perimeter \nIntersections",
       subtitle = "Count of Incident Intersections per Fishnet Cell | 2015-2019")+mapTheme()

```

### Map: Historical Fire Occurence (Yes or No) per Fishnet Cell
This is a map of hte outcome variable, `prev_fire`. It is clear that incidents of fires over the past five years were distributed throughout the area, but especially concentrated around the three middle counties: Ventura, Los Angeles, and Orange.

```{r yes or no numeric - fires by fishnet, message=FALSE, warning=FALSE}
# mapping fire incident - yes or no (1 or 0)

#convert to character first
fires_fishnet$prev_fire <- as.character(fires_fishnet$prev_fire)

ggplot() +
  geom_sf(data = fires_fishnet, aes(fill = prev_fire)) +
  geom_sf(data=socal_counties_coastal, aes(fill=NA))+
  scale_fill_manual(values=palette2)+
  labs(title = "Presence of Historical Fire Incidents in Southern California",
       subtitle="Per Fishnet Cell | 2015-2019 \n \n0=no fire occurred; 1=at least one fire occurred",
       fill="Fire Occurrence",
      caption="Data Sources: CALFIRE; California State Geoportal") +
  mapTheme()+ theme(legend.position='right')

```

## Exploring Raster Data: Vegetation, Impervious Surfaces, Elevation
Due to the size of raster files covering the entire state of California, ArcMap is used to do some initial data wrangling. A fishnet was created with 5000x5000m grid cells to use as the input zones for calculations performed for the following three variables. After these calculations were completed, they were joined back to each unique fishnet grid cell and a shapefile was exported to use in RStudio.

### Feature: Vegetation Majority
[Text here about vegetation risk to wildfire]

This **Vegetation** (`VEG_MAJORITY`) variable is built from raster representation of statewide vegetation from  [SOURCE]. In ArcMap, znal statistics as table was used to calculate the majority vegetation life-form type per grid cell of the fishnet.

```{r vegetation map, message=FALSE, warning=FALSE}
# map the majority vegetation classification by fishnet cell
ggplot() +
  geom_sf(data = fishnet_features, aes(fill = VEG_MAJORITY)) +
  scale_fill_manual(values = paletteInferno8)+
  labs(title = "Majority Vegetation Classification in Southern California",
       subtitle="Per Fishnet Cell",
       caption="Data source: frap.fire.ca.gov - FVEG geodatabase (2015)") + mapTheme()

```


### Feature: Mean Percent of Impervious Surface
[Text here about impervious surface risk to wildfire, esp in urban areas]

This **Impervious Surfaces** (`IMPERV_SURFACE_PCT_MEAN`) variable is derived from a raster file from the National Land Cover Database containing pixel data on surfaces classified as impervious. In ArcMap, znal statistics as table was used to calculate the mean percent impervious surface per grid cell of the fishnet.

```{r impervious surface map, message=FALSE, warning=FALSE}

ggplot() +
  geom_sf(data = fishnet_features, aes(fill = IMPERV_SURFACE_PCT_MEAN)) +
  scale_fill_viridis(option="B")+
  labs(title = "Mean Impervious Surface across Southern California",
       subtitle="Per Fishnet Cell",
       fill="Mean Percent of \nImpervious Surface",
       caption="Data source: National Land Cover Database (2016)") + mapTheme()

```

### Feature: Mean Elevation

[Text here about elevation risk to wildfire]

To create this **Elevation** (`ELEVATION_MEAN`) variable, a raster image of Digital Elevation Model of 15 arc-seconds resolution from the U.S. Geological Survey was brought into ArcMap. Zonal statistics as table was used to calculate the mean elevation per grid cell of the fishnet.

```{r elevation map, message=FALSE, warning=FALSE}
# map the mean elevation by fishnet cell
# source: https://geodata.lib.berkeley.edu/catalog/stanford-wj406qc5431; publisher - USGS
ggplot() +
  geom_sf(data = fishnet_features, aes(fill = ELEVATION_MEAN)) +
  scale_fill_viridis(option="B")+
  labs(title = "Mean Elevation across Southern California",
       subtitle="Per Fishnet Cell",
       fill="Mean Elevation \n(meters)",
       caption="Data source: USGS - Digital Elevation Model of North America, 15 arc-seconds resolution (2009)") + mapTheme()
```


```{r convert prev_fire to int, message=FALSE, warning=FALSE}
#Convert prev_fire to integer to prepare for model development later
fires_fishnet$prev_fire <- as.numeric(fires_fishnet$prev_fire)
```

## Engineering Weather Data Features
[Jenna: Discuss process of collecting, aggregating, and interpolating for the selected weather variables over the past five years.]

```{r load in weather data, results="hide"}
#load in specific weather data selection for the stations
#JENNA
weather <- read.csv("C:/Users/jenna/Documents/GitHub/MUSA508-final/data/weather_data_socal_2015to2019.csv") 

weather_annual <- weather %>% st_as_sf(coords = c("lon", "lat"), crs = 4236, agr="constant") %>% st_transform(st_crs("EPSG:3310"))

#JEFF - add local file path
#weather <- read.csv("C:/Users/jenna/Documents/GitHub/MUSA508-final/data/weather_data_socal_2015to2019.csv")
```


```{r load in weather data2, message=FALSE, warning=FALSE}
library(dplyr)
weather_annual <- weather_annual %>% dplyr::group_by(year, weather_station_id) %>%
      dplyr::summarize(mean_ann_tmpf = mean(mean_tmpf, na.rm = TRUE),
                       mean_ann_precipitation = mean(mean_precipitation, na.rm = TRUE),
                       mean_ann_humidity = mean(mean_humidity, na.rm = TRUE),
                       mean_ann_windspeed = mean(mean_wind_Speed, na.rm = TRUE))

```

[Jenna: Text here - creating new data frame for data averages by year per station to use with interpolation. Text about inverse distance weighted interpolation for four variables: precipitation, wind speed, temperature, humidity.]

```{r interpolation - precipitation, results='hide'}

#create new df with only precipitation variable
weather_precip <- weather_annual %>% dplyr::select(geometry, mean_ann_precipitation)

# perform idw for precipitation surface. DW interpolation that uses a power parameter of 2 (idp=2.0).
weather_precip.idw <- gstat::idw(mean_ann_precipitation ~ 1, weather_precip, newdata=fishnet_features, idp=2.0)

#rename var1.pred
weather_precip.idw <- weather_precip.idw %>% dplyr::rename(avg_precip_idw = var1.pred)

#create new df
precip.idw <- weather_precip.idw %>% dplyr::select(avg_precip_idw, geometry) 

#join to main fishnet
fishnet_precip <- st_join(fires_fishnet, precip.idw, join=st_equals)

```

```{r interpolation - avg wind speed, results='hide'}
library(dplyr)
#new df with only wind speed variable
weather_wind <- weather_annual %>% dplyr::select(geometry, mean_ann_windspeed)

#idw for wind speed surface. DW interpolation that uses a power parameter of 2 (idp=2.0).
weather_wind.idw <- gstat::idw(mean_ann_windspeed ~ 1, weather_wind, newdata=fishnet_features, idp=2.0)

#rename var1.pred
weather_wind.idw <- weather_wind.idw %>% dplyr::rename(avg_windspeed_idw = var1.pred)

avg_windspeed.idw <- weather_wind.idw %>% dplyr::select(avg_windspeed_idw, geometry) 

#join to precip fishnet, create a final fishnet
fishnet_with_wind <- st_join(fishnet_precip, avg_windspeed.idw, join=st_equals)
```

```{r interpolation - avg temp, results='hide'}
library(dplyr)#new df with only temp variable
weather_temp <- weather_annual %>% dplyr::select(geometry, mean_ann_tmpf)

#idw for temp surface. DW interpolation that uses a power parameter of 2 (idp=2.0).
weather_temp.idw <- gstat::idw(mean_ann_tmpf ~ 1, weather_temp, newdata=fishnet_features, idp=2.0)

#rename var1.pred
weather_temp.idw <- weather_temp.idw %>% dplyr::rename(avg_temp_idw = var1.pred)

avg_temp.idw <- weather_temp.idw %>% dplyr::select(avg_temp_idw, geometry) 

#join to wind fishnet, create a final fishnet
fishnet_with_temp <- st_join(fishnet_with_wind, avg_temp.idw, join=st_equals)
```

```{r interpolation - avg hum, results='hide'}
library(dplyr)
#new df with only humidity variable
weather_hum <- weather_annual %>% dplyr::select(geometry, mean_ann_humidity)

#idw for temp surface. DW interpolation that uses a power parameter of 2 (idp=2.0).
weather_hum.idw <- gstat::idw(mean_ann_humidity ~ 1, weather_hum, newdata=fishnet_features, idp=2.0)

#rename var1.pred
weather_hum.idw <- weather_hum.idw %>% dplyr::rename(avg_hum_idw = var1.pred)

avg_hum.idw <- weather_hum.idw %>% dplyr::select(avg_hum_idw, geometry) 

#join to wind fishnet, create a final fishnet
fishnet_with_hum <- st_join(fishnet_with_temp, avg_hum.idw, join=st_equals)
```

### Feature Maps: All Weather Features
These maps show the four engineered weather across coastal southern California. [Jenna: ADD MORE.]

```{r all weather features maps, results='hide'}
#maps with weather features

# TO DO - GRID ARRANGE BY 4
#precip
ggplot()+
  geom_sf(data=weather_precip.idw, aes(fill=avg_precip_idw)) +
  scale_fill_viridis(option="B") +
  labs(title="Average Precipitation across Southern California",
       fill="Avg Precip)",
  subtitle="Inverse Distance Weighted Interpoloation - Weather Station to Fishnet Cell",
  caption="Data source: ASOS, 2015-2019 Averages")+
          mapTheme()

#wind
ggplot()+
  geom_sf(data=weather_wind.idw, aes(fill=avg_windspeed_idw)) +
  scale_fill_viridis(option="B") +
  labs(title="Average Wind Speed across Southern California",
       fill="Avg Wind Speed \n(tenths of m/sec)",
  subtitle="Inverse Distance Weighted Interpoloation - Weather Station to Fishnet Cell",
  caption="Data source: ASOS, 2015-2019 Averages")+
          mapTheme()

#temp
ggplot()+
  geom_sf(data=weather_temp.idw, aes(fill=avg_temp_idw)) +
  scale_fill_viridis(option="B") +
  labs(title="Average Temperature across Southern California",
       fill="Avg Temperature",
  subtitle="Inverse Distance Weighted Interpoloation - Weather Station to Fishnet Cell",
  caption="Data source: ASOS")+
          mapTheme()

#humidity
ggplot()+
  geom_sf(data=weather_hum.idw, aes(fill=avg_hum_idw)) +
  scale_fill_viridis(option="B") +
  labs(title="Average Humidity across Southern California",
       fill="Avg Humidity",
  subtitle="Inverse Distance Weighted Interpoloation - Weather Station to Fishnet Cell",
  caption="Data source: ASOS")+
          mapTheme()


```


```{r rename to fishnet_final, warning=FALSE}
#rename to final fishnet
fishnet_final <- fishnet_with_hum

```


## Additional Feature Engineering

### Feature: Tree Mortality Hazard Classification
[Jenna: Text here.]

```{r tier1 tree mortality feature, message=FALSE, warning=FALSE}
tier1hazard_treemortality <- st_read("https://opendata.arcgis.com/datasets/a71a85136b0b414ea734fdfbe3d7674a_0.geojson") %>% st_transform(st_crs(fishnet_final))

## intersections of tier 1 areas with each fishnet cell, using length function
fishnet_final <- fishnet_final %>% mutate(n_tier1hazard_int = lengths(st_intersects(fishnet_final, tier1hazard_treemortality)))

```


### Feature: Distance to nearest electric substation
[Jenna: Text here.]

```{r distance to nearest electric substation, message=FALSE, warning=FALSE}
#more feature engineering...
#read in electric substations
electric_substations <- st_read("https://opendata.arcgis.com/datasets/7f37f2535d3144e898a53b9385737ee0_0.geojson") %>% 
  dplyr::select(Y = Latitude, X = Longitute) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet_final))

# calculate distance (meters) from each cell centroid to nearest electric substation and add as a variable
fishnet_final <-
  fishnet_final %>%
    mutate(dist_elec_substation=st_nn(st_centroid(fishnet_final), electric_substations, 1))

fishnet_final$dist_elec_substation <- as.numeric(fishnet_final$dist_elec_substation)  # Convert variable to numeric
```

```{r map distance to nearest electric substation, message=FALSE, warning=FALSE}
ggplot()+
  geom_sf(data=fishnet_final, aes(fill=dist_elec_substation)) +
  scale_fill_viridis(option="B") +
  labs(title="Distance to Nearest Electrical Substation in Southern California",
       fill="Distance \n(meters)",
  subtitle="Per Fishnet Cell",
  caption="Data source: California Open Data Portal")+
          mapTheme()

```

## Feature: Distance to nearest CALFIRE or contract fire facility
[Jenna: Text here.]

```{r dist to fire facility, message=FALSE, warning=FALSE}
#read in facilities
fire_facilities <- st_read("https://opendata.arcgis.com/datasets/1c8a93cac92f418e98a8fa6a2eaf4265_0.geojson") %>% 
  dplyr::select(Y = LAT, X = LON) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet_final))

# calculate distance (meters) from each cell centroid to nearest facility and add as a variable
fishnet_final <-
  fishnet_final %>%
    mutate(dist_fire_facility=st_nn(st_centroid(fishnet_final), fire_facilities, 1))

fishnet_final$dist_fire_facility <- as.numeric(fishnet_final$dist_fire_facility)  # Convert variable to numeric
```

```{r map dist to fire facility, message=FALSE, warning=FALSE}
ggplot()+
  geom_sf(data=fishnet_final, aes(fill=dist_fire_facility)) +
  scale_fill_viridis(option="B") +
  labs(title="Distance to Nearest Fire Facility in Southern California",
       fill="Distance \n(meters)",
  subtitle="Per Fishnet Cell",
  caption="Data source: California Open Data Portal")+
          mapTheme()

```


## Exploring the Spatial Process of Fire Incidents
After looking back at the outcome variable map of where the fire incidents occurred, we wanted to explore the distribution of fire incidents across space. 

### Local Moran’s I

The Local Moran’s I statistic is calculated to explore local spatial autocorrelation of fire incidents. It explores spatial clustering by measuring how similar locations are to their immediate neighbors. High Local Moran’s I values indicate that there is evidence of local clustering.

The results of the Local Moran’s I are plotted below, alongside the count of incidents (on the far left). These three outputs mapped include the Local Moran’s I, the p-value, and the significant hotspots (cells with higher local incident counts than expected).

In the Local Moran’s I map, evidence of spatial clustering exists especially in the center areas of the city, as evidenced by the higher values for cells. The p-value and significant hotspots maps further support this notion of local clustering.

```{r local morans i, message=FALSE, warning=FALSE}
library(spdep)
#use {spdep}
fishnet_final.nb <- poly2nb(as_Spatial(fishnet_final), queen=TRUE)
## and neighborhoods to list of weights
fishnet_final.weights <- nb2listw(fishnet_final.nb, style="W", zero.policy=TRUE)

fishnet_final.localMorans <- 
  cbind(
    as.data.frame(localmoran(fishnet_final$n_fires_int, fishnet_final.weights)),
    as.data.frame(fishnet_final)) %>% 
    st_sf() %>%
      dplyr::select(n_fires_int_Count = n_fires_int, 
                    Local_Morans_I = Ii, 
                    P_Value = `Pr(z > 0)`) %>%
      mutate(Significant_Hotspots = ifelse(P_Value <= 0.05, 1, 0)) %>%
      gather(Variable, Value, -geometry)

```

```{r spatial processes, eval=FALSE, message=FALSE, warning=FALSE}
vars <- unique(fishnet_final.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(fishnet_final.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(option="B", name="") +
      labs(title=i) +
      mapTheme() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Number of Fire Intersections"))

```

```{r local morans in fishnet, message=FALSE, warning=FALSE}
# create Local Moran's I feature in fishnet_final
fishnet_final <-
  fishnet_final %>% 
  mutate(n_fires_int.isSig = 
           ifelse(spdep::localmoran(fishnet_final$n_fires_int, 
                             fishnet_final.weights)[,5] <= 0.0000001, 1, 0))

## there was an issue here with nn function from class, using st_nn instead
fishnet_final <-
  fishnet_final %>% 
  mutate(n_fires_int.isSig.dist = 
           st_nn(st_centroid(fishnet_final),(st_centroid(filter(fishnet_final, n_fires_int.isSig == 1))), 1))


# Convert variable to numeric
fishnet_final$n_fires_int.isSig.dist <-as.numeric(fishnet_final$n_fires_int.isSig.dist)

```

In order to develop a model that generalizes well, Local Moran’s I is added as a feature in the final fishnet grid, using a dummy variable to indicate that a cell is part of a significant cluster. Then, the average nearest neighbor distance is measured from the centroid of each grid cell to its nearest significant cluster. The p-value chosen is 0.0000001 - the smaller the p-value, the more significant the clusters.

### Nearest Neighbor Distance to Hotspots
The distance to highly significant hotspots for fire incidents is shown in this map below.

```{r nn distance to hotspots map, message=FALSE, warning=FALSE}
ggplot() +
      geom_sf(data = fishnet_final, aes(fill=n_fires_int.isSig.dist), colour=NA) +
      scale_fill_viridis(option="B", direction = 1, 
                         name="st_nn \ndistance") +
      labs(title="Distance to Highly Significant Fire Hotspots") +
      mapTheme()
```

```{r fire_yesno, message=FALSE, warning=FALSE, include=FALSE}
#in preparation for the model development, the a new fire variable created to a yes/no variable.
fishnet_final <- fishnet_final %>% mutate(fire_yesno = case_when(fishnet_final$prev_fire == 1 ~ "yes",
                                                                 fishnet_final$prev_fire == 0 ~ "no"))

```

# Building the Model
[Jenna: Text here.]

```{r prev_fire var to numeric, message=FALSE, warning=FALSE}
# Convert variable to numeric
fishnet_final$prev_fire <- as.numeric(fishnet_final$prev_fire)  
```

```{r reg, message=FALSE, warning=FALSE}
set.seed(3456)
trainIndex <- createDataPartition(fishnet_final$prev_fire, p = .70,
                                list = FALSE,
                               times = 1)
dataTrain <- fishnet_final[ trainIndex,]
dataTest  <- fishnet_final[-trainIndex,]

reg.binomial<- glm(prev_fire ~ .,
                  data=st_drop_geometry(fishnet_final) %>% dplyr::select(-fire_yesno, -uniqueID, -n_fires_int,), family="binomial")

reg.binomial.r2 <- pR2(reg.binomial)[4]
summary(reg.binomial)
print(reg.binomial.r2)

```

## Model Results & Goodness of Fit
[Jenna: Text here.]

### Results

```{r reg stats, message=FALSE, warning=FALSE}
summ(reg.binomial)

```

### Confusion Matrix and Statistics
The confusion matrix and its statistics for the regression are shown below. The matrix of 0s and 1s shows the comparison between the observed and predicted instances of fire occurrence at the 50% threshold. As shown by the sensitivity and specificity..[ADD MORE]

```{r reg testprobs, message=FALSE, warning=FALSE}
testProbs <- data.frame(Outcome = as.factor(dataTest$prev_fire),
                        COUNTY_NAME = dataTest$COUNTY_NAME,
                        Probs = predict(reg.binomial, dataTest, type= "response"))

testProbs <- 
  testProbs %>%
  mutate(predOutcome  = as.factor(ifelse(testProbs$Probs > 0.5 , 1, 0)))
```

```{r reg testprobs confmatrix, message=FALSE, warning=FALSE}
xtab.reg1 <-caret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, 
                       positive = "1")

as.matrix(xtab.reg1) %>% kable(caption = "Confusion Matrix") %>% kable_styling("striped", full_width = T, font_size = 14, position = "left")
```

```{r reg testprobs confmatrix statistics, message=FALSE, warning=FALSE}
as.matrix(xtab.reg1, what="classes") %>% kable(caption = "Confusion Matrix - Statistics") %>% kable_styling(font_size = 14, full_width = T,
                bootstrap_options = c("striped", "hover"))
```

### Distribution of Predicted Probabilities
The visualization below shows the distribution of outcomes for the model. It is clear that the model is better at predicting 0s (the “no fire occurred” outcome) than predicting 1s (the “at least one fire occurred” outcome).

```{r reg testprobs ggplot, message=FALSE, warning=FALSE}

ggplot(testProbs, aes(x = Probs, fill = as.factor(Outcome))) + 
  geom_density() +
  facet_grid(Outcome ~ .) +
  scale_fill_manual(values = palette2) + xlim(0, 1) +
  labs(x = "Fire Occurred or Not (outcome variable)", y = "Density of Probabilities",
       title = "Distribution of Predicted Probabilities by Observed Outcome") +
  plotTheme() + theme(strip.text.x = element_text(size = 18),
        legend.position = "none")

```

### ROC Curve
The Receiver Operating Characteristic (ROC) Curve for the model is shown below. This curve is a helpful goodness of fit indicator, while helping to visualize trade-offs between true positive and false positive metrics at each threshold from 0.01 to 1. A line going “over” the curve indicates a useful fit. According to this curve...[ADD MORE]

The area under the curve (AUC) here is about .79, indicating a fairly useful fit. A reasonable AUC is between 0.5 and 1.

```{r testProbs ROC, message=FALSE, warning=FALSE}
ggplot(testProbs, aes(d = as.numeric(Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "Receiver Operating Characteristic (ROC) Curve", subtitle = "AUC = ~0.79") + plotTheme()

```

```{r AUC, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
pROC::auc(testProbs$Outcome, testProbs$Probs) %>% kable(caption = "Area Under the Curve (AUC)") %>% kable_styling(position ="left", full_width = T, font_size = 18)
```

## Cross Validation (CV)

### K-fold CV
To assess the generalizability of the models, each one is cross-validated. The factor levels for the outcome variable (“yes” and “no”) are reversed at this time to ensure that the cross-validation reads “yes” before “no” (reverse-alphabetical order) and calculates the metrics appropriately.


```{r cv, message=FALSE, warning=FALSE}
ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)

str(fishnet_final$fire_yesno)
fishnet_final$fire_yesno <- as.factor(fishnet_final$fire_yesno) 
fishnet_final$fire_yesno <- fct_rev(fishnet_final$fire_yesno)

cvFit.reg1 <- train(fire_yesno ~ .,
                  data=st_drop_geometry(fishnet_final) %>% dplyr::select(-prev_fire, -uniqueID, -n_fires_int), 
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl)
```

### K-fold CV Results & Goodness of Fit

As with the model above, when cross-validated, it performs better when predicting negatives than it does predicting positive occurrences of fire. While this may initially seem problematic, the ultimate use case for this information will be the allocation of evacuation preparedness kits. Looking at the benefit for the public good, the "cost" of predicting falsely is not really a cost for the community receiving the kit -- preparation materials can be useful in the face of other disasters. 

```{r cv results}
print(cvFit.reg1) %>% kable(caption="Cross-Validated Results") %>% kable_styling(font_size = 14, full_width = T,bootstrap_options = c("striped", "hover"))

```

These cross-validation goodness of fit metrics are shown in the faceted plots below. These include the mean for area under the curve (“ROC” in this function’s output), sensitivity (“Sens”), and specificity (“Spec”) across all of the 100 folds.

To evaluate the generalizability of the model, we look to the distributions for each of the goodness of fit metrics. Tighter distributions around the means are indicative of greater generalizability. The model generalize well for specificity, meaning that it correctly predict instances of no fire occurring most cases. On the other hand, they do not generalize as well for sensitivity, or the rate at which they correctly predict a fire occurrence. 


```{r cv goodness of fit metrics plots, message=FALSE, warning=FALSE}
dplyr::select(cvFit.reg1$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit.reg1$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) +
    geom_histogram(bins=35, fill = "#f7b61f") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "red", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics",
         subtitle = "Across-fold mean reprented as dotted lines")
```

## Costs and Benefits of Identifying False Positives and Negatives

Our model will not have perfect accuracy, and when determining the optimal threshold it’s important to consider the costs and benefits of inaccurately predicting data. Keep in mind that the ultimate use case for this data is to help inform where emergency supply kits are allocated. For the sake of this section, let’s make some approximations and assume the following:

* Emergency supply kits cost $50
* Predictions are being made at the cell-level
* Kits are being distributed to every household in a cell

Type of prediction | What this means | Total cost | Total benefit
------------- | ------------- | ------------- | -------------
True Positives | We predict a fire and there is one | Households in this tract are more likely to be prioritized for a kit, costing $50/household | These households will be better prepared for a future evacuation
False Positives | We predict a fire and there is not one | **Households in this tract are more likely to be prioritized for a kit, costing $50/household** | No immediate benefit, although these kits do not expire after a year and these households will be better prepared for a future evacuation
True Negatives | We do not predict a fire and there is not one | Households here are less likely to be prioritized for a kit | None
False Negatives | We do not predict a fire and there is one | Households here are less likely to be prioritized for a kit, and **these households will be less prepared for a fire and at greater risk for evacuation** | None

As the table illustrates, optimizing the algorithm for more false positives will incur a greater financial cost and these supply kits may not go to immediate use. Whereas, optimizing for more false negatives is potentially leaving more households underprepared for an evacuation. For that reason, from a social and moral perspective, we believe that the ultimate cost of false positives is less than the cost of false negatives.

This is simply for illustrative purposes. As our final algorithm ends up extrapolating probabilities to the tract-level and then combining this data with the social vulnerability index, we cannot make any quantitative statements about cost-benefit at this time.


## Finding the Optimal Threshold
In order to best optimize this model for use, identifying the threshold that provides the greatest accuracy is important. This is done by running the `iterateThresholds` function and looking at teh results for all thresholds the table of thresholds 0.01 through 0.99. The optimal threshold in this case is 0.55.

```{r iterate threshold function, message=FALSE, warning=FALSE}
iterateThresholds <- function(data, group) {
   group <- enquo(group)
  x = .01
  all_prediction <- data.frame()
  while (x <= 1) {
  
  this_prediction <-
      testProbs %>%
      mutate(predOutcome = ifelse(Probs > x, 1, 0)) %>%
         group_by(!!group) %>%
      dplyr::count(predOutcome, Outcome) %>%
      dplyr::summarize(sum_TN = sum(n[predOutcome==0 & Outcome==0]),
                sum_TP = sum(n[predOutcome==1 & Outcome==1]),
                sum_FN = sum(n[predOutcome==0 & Outcome==1]),
                sum_FP = sum(n[predOutcome==1 & Outcome==0]),
            total=sum(n)) %>%
    mutate(True_Positive = sum_TP / total,
         True_Negative = sum_TN / total,
         False_Negative = sum_FN / total,
         False_Positive = sum_FP / total,
         Accuracy = (sum_TP + sum_TN) / total, Threshold = x)
  
  all_prediction <- rbind(all_prediction, this_prediction)
  x <- x + .01
  }
return(all_prediction)
}

```


```{r threshold table, message=FALSE, warning=FALSE}
whichThreshold <- iterateThresholds(testProbs)

allThresholds<-kable(whichThreshold) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)%>%
  scroll_box(width = "100%", height = "500px")

allThresholds
```


### How Does Ths Model Perform Across Counties?

To assess how well this model performs across space, goodness of fit metrics are calculated across counties.

This visualization below shows the prediction outcomes grouped by county, at the optimal threshold identified (0.55). Accuracy rate is the highest for Los Angeles and San Diego counties. Los Angeles County's negative rate is the second-lowest (behind Ventura). Due to its greater accuracy - coupled with the known degree of population density - Los Angeles presents an compelling space to dive in further when developing our application for evacuation kit distribution. 

```{r counties - confusion matrix metrics, message=FALSE, warning=FALSE}

testProbs.thresholds <- iterateThresholds(data=testProbs, group=COUNTY_NAME)

filter(testProbs.thresholds, Threshold == "0.55")%>%
    dplyr::select(Accuracy, COUNTY_NAME, True_Positive, True_Negative, False_Negative, False_Positive) %>%
    gather(Variable, Value, -COUNTY_NAME) %>%
    ggplot(aes(Variable, Value, fill = COUNTY_NAME)) +
      geom_bar(aes(fill = COUNTY_NAME), position = "dodge", stat = "identity") +
      scale_fill_manual(values = paletteInferno5) +
      labs(title="Confusion Matrix Rates by County",
           subtitle = "55% threshold", x = "Outcome",y = "Rate") +
      plotTheme() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 


```

Here is an ROC plot showing the goodness of fit for each of the five counties evaluated. It measures trade-offs in trues positive and false positive rates for each threshold. Across thresholds, this plot conveys that the model performs similarly for Orange and San Diego counties, the two furthest southern ones in the study area. This indicates that there may be some spatial processes at play at these local scales, or some additional features not captured in the model. The model does not perform as well for Santa Barbara (as indicated by the closer "hugging" of the trend line). This is consistent with the confusion matrix metrics shown in the bars above. It seems that at this optimal threshold, the model is most accurate when looking around Los Angeles and Ventura.

```{r counties ROC, message=FALSE, warning=FALSE}
library(pROC)
#notworking
#aucTable <- aucTable %>%
 # group_by(COUNTY_NAME)%>% 
  #dplyr::summarize(AUC = auc(Outcome, Probs)) %>%
  #mutate(AUC = as.character(round(AUC, 3)))

ggplot(testProbs, aes(d = as.numeric(Outcome), m = Probs, group=COUNTY_NAME, color=COUNTY_NAME)) +
  geom_roc(n.cuts = 50, labels = F)+
  scale_color_manual(values = paletteInferno5) +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curves, by County") + plotTheme()

```


## Preparing for the Use Case: Extrapoloating from Grid Cells to Census Tracts
Now that we have an optimal threshold, we predict out the probabilities and join them back to the fishnet using each cell's unique ID. This new dataset is then exported, and  data set, we use areal weighting to make estimates from a set of source polygons (the grid cells) to the overlapping, congruent set of target polygons(census tracts). The census tract geometries are incorporated using the US Census Bureau's API.


```{r predict fishnet, eval=FALSE, message=FALSE, warning=FALSE}
predict_fishnet <- 
  data.frame(uniqueID = fishnet_final$uniqueID,
  Outcome = fishnet_final$prev_fire,
  Probs = predict(reg.binomial, fishnet_final, type="response"))

predict_fishnet <- 
  predict_fishnet %>%
  mutate(predOutcome  = as.factor(ifelse(predict_fishnet$Probs > 0.55, 1, 0)))


fishnet_final_preds <- cbind(fishnet_final,predict_fishnet)
```

```{r census tracts, message=FALSE, warning=FALSE}
library(tidycensus)
library(dplyr)
projection <- "EPSG:3310"

#Jenna
census_api_key("41e1c0d912341017fa6f36a5da061d3b23de335e", overwrite=TRUE)

#Jeff
#census_api_key("e59695f18b5f5959947fd9098feba458ca285cc5", overwrite=TRUE)

acs_vars <- c("B25026_001E","B02001_002E","B15001_050E",
                         "B15001_009E","B19013_001E","B25058_001E",
                         "B06012_002E", "B01001_048E", "B01001_049E",
                         "B01001_024E", "B01001_025E", "B07013_002E")

countiesOfInterest <- c('Los Angeles', 'Ventura', 'Orange', 'San Diego', 'Santa Barbara')

tracts17 <-
    get_acs(geography = "tract", 
            variables = acs_vars, 
            year=2017,
            state='CA', 
            county=countiesOfInterest,
            geometry=T, 
            output="wide") %>%
        st_transform(projection) %>%
        dplyr::rename(TotalPop = B25026_001E,
               White = B02001_002E,
               MedianHouseholdIncome = B19013_001E,
               MedianRent = B25058_001E,
               Poverty = B06012_002E,
               HousingUnits = B07013_002E) %>%
        dplyr::select(-NAME, -starts_with("B")) %>%
        mutate(pctNonWhite = ifelse(TotalPop < White, 0, ifelse(TotalPop > 0, 1 -(White / TotalPop),0)),
               pctPoverty = ifelse(TotalPop > 0, Poverty / TotalPop, 0))


tracts17 <- tracts17 %>% filter(GEOID != '06037599100') %>% filter(GEOID != '06037599000') %>% filter(GEOID != '06083990000') %>% filter(GEOID != '06111980000') %>% filter(GEOID != '06083980100') %>% filter(GEOID != '06111990100')

tracts17 <- tracts17 %>% st_transform(st_crs(fishnet_final))

#tracts17_LA <-
#    get_acs(geography = "tract", 
#            variables = acs_vars, 
#            year=2017,
#            state='CA', 
#            county="Los Angeles",
#            geometry=T, 
#            output="wide") %>%
#        st_transform(projection)

```

```{r message=FALSE, warning=FALSE}
#Code - ran separately, but this shows the process

#fishnet_final_preds_only <- fishnet_final_preds %>% dplyr::select(uniqueID, Probs, geometry, COUNTY_NAME)

#interpolate_probs <-
#  dplyr::select(fishnet_final_preds, Probs) %>%
#  st_interpolate_aw(., tracts17, extensive = FALSE) %>% st_sf()

#interpolation_tracts <- st_join(interpolate_probs, tracts17, join=st_equals)

#fireProb_tracts <- interpolation_tracts %>% dplyr::select(GEOID, geometry, Probs) %>% st_sf()

#fireProb_tracts_nogeom <- interpolation_tracts %>% dplyr::select(GEOID, Probs) %>% st_drop_geometry()

#write.csv
```


# Incorporating the Social Vulnerability Index

## Assessing Population Vulnerability
Our final algorithm for allocating emergency supply kits will not only take into account the probability of a fire occurring. We will also prioritize census tracts to receive aid if their populations are more "vulnerable" to an emergency like a wildfire. These populations may be less likely to be able to evacuate or less likely to be able to purchase/own an emergency supply kit on their own.

We initially explored creating our own composite score, but decided to use the reputable CDC Social Vulnerability Index (https://www.atsdr.cdc.gov/placeandhealth/svi/index.html). The SVI uses variables from the American Community Survey. Among its many uses, CDC recommends using the score to "allocate emergency preparedness funding by community need."


```{r setup2}
library(sf)
library(ggplot2)
library(tidycensus)
library(dplyr)
palette5 <- c("#f0f9e8","#bae4bc","#7bccc4","#43a2ca","#0868ac")
projection <- "EPSG:6423"
options(scipen = 999)

```

```{r getVulnIndexScore}
vulnIndex <- st_read("California-SVI/SVI2018_CALIFORNIA_tract.shp")%>%
  filter(COUNTY == 'Los Angeles') %>%
  filter(FIPS != '06037599100') %>%
  filter(FIPS != '06037599000') %>%
  dplyr::select(starts_with("RPL_"),"COUNTY","FIPS","LOCATION","E_TOTPOP") %>%
  mutate_if(is.numeric, list(~na_if(., -999)))%>%
  st_transform(projection) 

```

The Social Vulnerability Index is made up of 4 sub-themes: 

```{r fig.cap="Image from CDC"}
knitr::include_graphics("C:/Users/jenna/Documents/GitHub/MUSA508-final/SVI_image.jpg")
```

We fetched the data for each of these themes for each census tract in Los Angeles County. These plots show the tracts' percentile rankings for the sub-themes as well as the primary composite score.

```{r exploratoryPlots}
svi_theme1 <- ggplot(vulnIndex) +
  geom_sf(data = st_union(vulnIndex))+
  geom_sf(aes(fill = q5(RPL_THEME1)),lwd = 0) +
  scale_fill_manual(values = palette5,
                    labels = qBr(vulnIndex %>% drop_na(RPL_THEME1), "RPL_THEME1", rnd=FALSE),
                    name = "Percentile") +
  labs(title = "Socioeconomic status", subtitle = "By census tract, 2018 CDC Social Vulnerability Index") +
  mapTheme() + 
  theme(plot.title = element_text(size=16))

svi_theme2 <- ggplot(vulnIndex) +
  geom_sf(data = st_union(vulnIndex))+
  geom_sf(aes(fill = q5(RPL_THEME2)),lwd = 0) +
  scale_fill_manual(values = palette5,
                    labels = qBr(vulnIndex %>% drop_na(RPL_THEME2), "RPL_THEME2", rnd=FALSE),
                    name = "Percentile") +
  labs(title = "Household composition & disability", subtitle = "By census tract, 2018 CDC Social Vulnerability Index") +
  mapTheme() + 
  theme(plot.title = element_text(size=16))

svi_theme3 <- ggplot(vulnIndex) +
  geom_sf(data = st_union(vulnIndex))+
  geom_sf(aes(fill = q5(RPL_THEME3)),lwd = 0) +
  scale_fill_manual(values = palette5,
                    labels = qBr(vulnIndex %>% drop_na(RPL_THEME3), "RPL_THEME3", rnd=FALSE),
                    name = "Percentile") +
  labs(title = "Minority status & language", subtitle = "By census tract, 2018 CDC Social Vulnerability Index") +
  mapTheme() + 
  theme(plot.title = element_text(size=16))

svi_theme4 <- ggplot(vulnIndex) +
  geom_sf(data = st_union(vulnIndex))+
  geom_sf(aes(fill = q5(RPL_THEME4)),lwd = 0) +
  scale_fill_manual(values = palette5,
                    labels = qBr(vulnIndex %>% drop_na(RPL_THEME4), "RPL_THEME4", rnd=FALSE),
                    name = "Percentile") +
  labs(title = "Housing type & transportation", subtitle = "By census tract, 2018 CDC Social Vulnerability Index") +
  mapTheme() + 
  theme(plot.title = element_text(size=16))

grid.arrange(svi_theme1, svi_theme2, svi_theme3, svi_theme4, nrow=2,ncol=2)

svi_total <- ggplot(vulnIndex) +
  geom_sf(data = st_union(vulnIndex))+
  geom_sf(aes(fill = q5(RPL_THEMES)),lwd = 0) +
  scale_fill_manual(values = palette5,
                    labels = qBr(vulnIndex %>% drop_na(RPL_THEMES), "RPL_THEMES", rnd=FALSE),
                    name = "Percentile") +
  labs(title = "Social Vulnerability Index Composite", subtitle = "By census tract, 208 CDC Social Vulnerability Index") +
  mapTheme() + 
  theme(plot.title = element_text(size=16))

svi_total
```

Although the SVI dataset includes population information for each tract, we want to be able to allocate one kit per household. We get household data from the American Community Survey and join it with our social vulnerability index information.

```{r getHousingUnitData, message=FALSE, warning=FALSE}
library(dplyr)
projection <- "EPSG:6423"

#census_api_key("e59695f18b5f5959947fd9098feba458ca285cc5", install=TRUE, overwrite=TRUE)
housingUnitsByTract <- get_acs(geography = "tract", variables = c("B07013_002E"), 
            year=2018, state='CA', county=c('Los Angeles'), geometry=T, output="wide") %>%
        st_transform(projection) %>%
        dplyr::rename(HousingUnits = "B07013_002E") %>%
        dplyr::select(-starts_with("B"), -NAME)  %>%
  filter(GEOID != '06037599100') %>%
  filter(GEOID != '06037599000') %>%
  dplyr::rename(FIPS='GEOID') %>%
  st_drop_geometry()

vulnIndex <- vulnIndex %>% dplyr::left_join(housingUnitsByTract, left=TRUE)

```

We combine the Social Vulnerability Index with our fire risk scores for each tract.

```{r combineScores, message=FALSE, warning=FALSE}
fireProbabilities <- read.csv("fireProb_tracts_nogeom.csv") %>%
    dplyr::rename(FIPS='GEOID') %>%
    mutate(FIPS=paste('0',as.character(FIPS),sep="")) %>%
    dplyr::rename(FireProb='Probs') 

fireProbabilities <- fireProbabilities %>%
    mutate(FireProbNormalized = (FireProb)/(max(fireProbabilities$FireProb)))

finalDataSet <- vulnIndex %>% dplyr::left_join(fireProbabilities, left=TRUE)

#finalDataSet <- st_write("EmergencyKitDistribution/finalDataSet-forShiny.shp")
```


Finally, we create a function that can be used to output a prioritized list of census tracts. The inputs for the function are:

1. Total allocated budget (default: $250,000)
2. Cost of the kit (default: $20)
3. Percentage of households in each tract to distribute a kit (default: 100%)
4. Amount of weight to give to fire risk score, vs. vulnerability index (default: 50%)

The function returns a data set of all the tracts with a column added to indicate whether or not they should receive kits.

```{r finalOutput, message=FALSE, warning=FALSE}

addComma <- function(num){ format(num, big.mark=",")}

getPrioritizedCensusTracts <- function(budget = 250000, costOfKit = 20, pctOfTract = 1.0, weight = 0.5){

  d <- finalDataSet %>%
    mutate(isReceivingKits = 0) %>%
    mutate(totalKitsToSend = 0) %>%
    mutate(priorityRanking = FireProbNormalized*weight + RPL_THEMES*(1-weight)) %>%
    arrange(desc(priorityRanking))

   numOfKits <- (budget / costOfKit)
  
  budgetRemaining <- budget
  tract.i <- 1
  totalHousingUnits <- 0
  
  while(budgetRemaining > costOfKit) {
      housingUnits <- floor(d[tract.i,]$HousingUnits * pctOfTract)
      if (housingUnits == 0) { tract.i <- tract.i+1; next; }
      costForTract <- housingUnits*costOfKit
   #   print(paste("Tract #",d[tract.i,]$FIPS, " is priority #", tract.i, " and has ", addComma(housingUnits), " housing units, Cost of kits: $",addComma(costForTract), sep=""))
      if(costForTract <= budgetRemaining) {
  #      print(paste("Budget remaining: $", addComma(budgetRemaining),sep=""))
        kitsSent <- housingUnits
      } else {
        possibleHousingUnits <- floor(budgetRemaining/costOfKit)
 #       print(paste("Cost for whole tract exceeds remaining budget, partial distribution to",addComma(floor(possibleHousingUnits))," housing units possible"))
        kitsSent <- possibleHousingUnits
      }
      budgetRemaining <- budgetRemaining - costForTract
      d[tract.i,]$isReceivingKits <- 1
      d[tract.i,]$totalKitsToSend <- kitsSent
      tract.i <- tract.i + 1  
  }
#  print(paste(addComma(totalHousingUnits),"housing units in", tract.i, "LA County tracts can be served by investing $",addComma(budget),"in the emergency kit allocation program (not including overhead costs)"))
  attr(d, "budget") <- budget
  attr(d, "costOfKit") <- costOfKit
  attr(d, "pctOfTract") <- pctOfTract
  attr(d, "weight") <- weight
  return(d)
}

```

This function uses the dataframe from getPriotizedCensusTracts() to generate a map of the tracts that can be served.

```{r plot, message=FALSE, warning=FALSE}
getServedTractsPlot <- function(df, ttl, st){
  allocationMap <- ggplot(df) +
    geom_sf(data = st_union(df))+
    geom_sf(aes(fill = isReceivingKits),lwd = 0) +
    labs(subtitle = st, title = ttl) +
    theme(plot.title = element_text(size=14)) +
    theme(plot.subtitle = element_text(size=10)) +
    theme(legend.position = "none")
  
  allocationMap
}
```


We are now able to see exactly which tracts are served. Here are a few examples where we're varying the budget and then the percentage of households in a tract that we send a kit.

```{r demoFinalAlgorithm, message=FALSE, warning=FALSE}

example1 <- getPrioritizedCensusTracts(budget = 250000, costOfKit = 20)
example2 <- getPrioritizedCensusTracts(budget = 2500000, costOfKit = 20)
example3 <- getPrioritizedCensusTracts(budget = 2500000, costOfKit = 20, pctOfTract = 0.5)
example4 <- getPrioritizedCensusTracts(budget = 2500000, costOfKit = 20, pctOfTract = 0.5, weight=0)
example5 <- getPrioritizedCensusTracts(budget = 2500000, costOfKit = 20, pctOfTract = 0.5, weight=1)

example6 <- getPrioritizedCensusTracts(budget = 6100000, costOfKit = 20, pctOfTract = 0.8, weight=0.5)

sum(example6$totalKitsToSend)
sum(example6$totalKitsToSend)

example1.plot <- getServedTractsPlot(example1, "Tracts served (ex. 1)", "$250k and $20 kits")
example2.plot <- getServedTractsPlot(example2, "Tracts served (ex. 2)","Increased budget to $2.5m")
example3.plot <- getServedTractsPlot(example3, "Tracts served (ex. 3)","Sending kits to only 50% of households increases amount of served tracts")

grid.arrange(example1.plot, example2.plot, example3.plot, nrow=2)
```

We can also adjust the weights to see how different tracts would be served if we ONLY prioritized based off the vulnerability index or ONLY prioritized based off of the probability of a fire occurring.

**As this graphic makes clear, by only using wildfire tracts or only using census data, you’d be serving very different and distant households.**

```{r demoFinalAlgopt2}
example4.plot <- getServedTractsPlot(example4, "Tracts served (ex. 4)","Only prioritizing on vulnerability index")
example5.plot <- getServedTractsPlot(example5, "Tracts served (ex. 5)","Only prioritizing on fire probability")

grid.arrange(example4.plot, example5.plot, nrow=1)
```

# Interactive Application

For our Los Angeles County program manager, we have made this algorithm available via an interactive application. This application can be used at https://bit.ly/508wildfire or below. The manager can adjust the inputs for the algorithm and see a live map and table update to highlight the tracts to serve.

[EMBED]

# Discussion and Future Work
We are excited for the potential of our algorithm to strategically distribute emergency supply kits to the highest priority census tracts in Los Angeles. As seen above, the algorithm helps prioritize a very different group of households compared to just using wildfire risk or just using vulnerability index. The interactive applications gives a program manager the flexibility to explore this data in more detail to make an informed decision about how to spend their budget.

Reflecting on the fire probability algorithm, we believe that this is an instrumental part of the final product. We have used historical data to create a model that predicts with a high degree of accuracy where fires may occur. The model generalizes reasonably well to most other counties around Los Angeles. In future work, we’d want to continue to improve the generalizability of the model and examine how well it generalizes for different types of tracts (eg. tracts with lower and higher socioeconomic status).

We believe that our application can also be extended to serve other geographic areas, such as other Los Angeles County. Perhaps even more exciting is to think about the use cases that it may have for other allocating other types of emergency relief – such as financial aid or coronavirus vaccines. We believe that this is just the beginning. We hope that 508 Analytics can help communities make data-informed responses to emergency disasters and every day problems.

- Jenna Epstein and Jeff Stern, December 2020
